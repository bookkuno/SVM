{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the Datasets\n",
    "data1A = pd.read_csv(\"/Users/book_kuno/Downloads/DDoS 2018/02-20-2018.csv\", low_memory=False)\n",
    "data2A = pd.read_csv(\"/Users/book_kuno/Downloads/DDoS 2018/02-21-2018.csv\", low_memory=False)\n",
    "\n",
    "#-----------Customized part for each particular datasets--------------\n",
    "# List of columns to drop\n",
    "columns_to_drop = ['Flow ID', 'Src Port', 'Src IP', 'Dst IP']\n",
    "# Drop the specified columns from data1\n",
    "data1AD = data1A.drop(columns=columns_to_drop, errors='ignore')\n",
    "# # Randomly sample 1/10 of the data\n",
    "data1 = data1AD.sample(frac=0.01, random_state=42)  # frac=0.1 means 10%, random_state ensures reproducibility\n",
    "print(data1.head())\n",
    "data2 = data2A.sample(frac=0.1, random_state=42)  # frac=0.1 means 10%, random_state ensures reproducibility\n",
    "print(data2.head())\n",
    "#-----------Customized part for each particular datasets--------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Preprocess the Data\n",
    "def preprocess_data(data):\n",
    "    data.columns = data.columns.str.strip()\n",
    "    data = data.dropna()\n",
    "    \n",
    "    # Encode the target column ('Label')\n",
    "    encoder = LabelEncoder()\n",
    "    data['Label'] = encoder.fit_transform(data['Label'])\n",
    "    \n",
    "    # Select only numeric columns for scaling\n",
    "    numeric_columns = data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    numeric_columns.remove('Label')  # Exclude the target column\n",
    "    \n",
    "    # Check for infinite or extremely large values\n",
    "    data[numeric_columns] = data[numeric_columns].replace([np.inf, -np.inf], np.nan)\n",
    "    data = data.dropna(subset=numeric_columns)\n",
    "    \n",
    "    # Scale the numeric feature columns\n",
    "    scaler = StandardScaler()\n",
    "    data[numeric_columns] = scaler.fit_transform(data[numeric_columns])\n",
    "    \n",
    "    X = data[numeric_columns]\n",
    "    y = data['Label']\n",
    "    \n",
    "    # Update the return statement to include the encoder\n",
    "    return X, y, encoder\n",
    "\n",
    "# X1, y1 = preprocess_data(data1)\n",
    "# X2, y2 = preprocess_data(data2)\n",
    "X1, y1, encoder1 = preprocess_data(data2)\n",
    "X2, y2, encoder2 = preprocess_data(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Split the Training Data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X1, y1, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Train the Linear SVM\n",
    "svm_model = SVC(kernel='linear', C=100, class_weight='balanced')\n",
    "#function is used to create an SVM classifier (from scikit-learn)\n",
    "svm_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Validate the Model\n",
    "y_val_pred = svm_model.predict(X_val)\n",
    "print(\"Validation Results\")\n",
    "print(classification_report(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Hyperparameter Tuning (Optional)\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'kernel': ['linear']\n",
    "}\n",
    "grid_search = GridSearchCV(SVC(), param_grid, cv=3, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best Parameters:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Test the Model\n",
    "y_test_pred = svm_model.predict(X2)\n",
    "print(\"Test Results\")\n",
    "print(confusion_matrix(y2, y_test_pred))\n",
    "print(classification_report(y2, y_test_pred))\n",
    "print(\"Accuracy:\", accuracy_score(y2, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Plot the Predicted Data Points with Class Names\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib.colors import ListedColormap\n",
    "import numpy as np\n",
    "\n",
    "def plot_predicted_data_with_class_names(X, y_true, y_pred, encoder, title=\"Predicted Data Points\"):\n",
    "    # Reduce dimensionality to 2D for visualization using PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    \n",
    "    # Get all possible class labels from the test data and predictions\n",
    "    all_classes = np.union1d(np.unique(y_true), np.unique(y_pred))\n",
    "    class_names = encoder.inverse_transform(all_classes)\n",
    "    \n",
    "    # Define a color map for all possible classes\n",
    "    cm_bright = ListedColormap(['#FF0000', '#0000FF', '#00FF00', '#FFA500', '#800080', '#FFFF00', '#00FFFF', '#8B4513'])  # Add more colors if needed\n",
    "    num_classes = len(all_classes)\n",
    "    \n",
    "    # Create a scatter plot with predicted labels\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_pred, cmap=cm_bright, edgecolor='k', alpha=0.8)\n",
    "    \n",
    "    # Add a legend with class names\n",
    "    handles = [plt.Line2D([0], [0], marker='o', color='w', \n",
    "                          markerfacecolor=cm_bright(i / (num_classes - 1)), markersize=10)\n",
    "               for i in range(num_classes)]\n",
    "    plt.legend(handles, class_names, title=\"Classes\", loc=\"best\")\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"PCA Component 1\")\n",
    "    plt.ylabel(\"PCA Component 2\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Assuming X2, y2 (true test labels), y_test_pred (predictions), and encoder2 are already defined\n",
    "plot_predicted_data_with_class_names(X2, y2, y_test_pred, encoder2, title=\"SVM Predicted Data Points with Class Names\")\n",
    "\n",
    "''' \n",
    "Understanding the Graph\n",
    "1)Axes:\n",
    "The x-axis represents the first principal component (PC1).\n",
    "The y-axis represents the second principal component (PC2).\n",
    "2)Data Points:\n",
    "Each point on the graph represents an observation (e.g., a sample or a data point) in your dataset.\n",
    "The position of each point is determined by its scores on PC1 and PC2.\n",
    "3)Variance:\n",
    "PC1 captures the maximum variance in the data, meaning it explains the largest amount of variability.\n",
    "PC2 captures the second most variance, orthogonal to PC1, meaning it explains the next largest amount of variability without overlapping with PC1.\n",
    "\n",
    "Interpreting the Graph\n",
    "1)Clusters:If the points form distinct clusters, this suggests that there are natural groupings in your data. These clusters can indicate different categories or classes within your dataset.\n",
    "2)Spread:The spread of points along PC1 and PC2 indicates how much variance each component explains. A wide spread along PC1 means it captures a lot of the data's variability.\n",
    "3)Direction:The direction of the spread can give insights into the relationships between variables. For example, if points are spread diagonally, it suggests that the original variables contributing to PC1 and PC2 are correlated.\n",
    "4)Outliers:Points that are far from the main cluster can be outliers. These observations might have unique characteristics or errors.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
